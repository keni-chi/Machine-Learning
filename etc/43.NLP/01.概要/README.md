# NLP

## 概要
覚書である。順次記載予定。  


2.自然言語と単語の分散処理
    2.2シソーラス
        類語辞書。car = auto, automobile....
        2.2.1WordNet
            1985年にスタートしたシソーラス。
        2.2.2シソーラスの問題点
            時代の変化に対応するのが困難
            人の作業コストが高い
            単語の細かなニュアンスを表現できない
    2.3カウントベースの手法
        コーパスの使用。大量のテキストデータのこと。
        2.3.2単語の分散表現
            単語の意味を的確にとらえたベクトル表現。RGBなど。
        2.3.3分布仮説
            単語の意味は周囲の単語によって形成される
            ウインドウサイズが１の場合、左右の１単語。
        2.3.4共起行列
            周囲の単語をカウント。カウントベースの手法、統計的手法、という。
        2.3.5ベクトル間の類似度
            コサイン類似度
    2.4カウントベースの手法の改善
        2.4.1相互情報量(PMI)
            theのように高頻度の単語は関連性が高くならないようにする。carとdriveの関連が強い結果になる。
        2.4.2次元削減
            特異点分解（SVD)。任意の行列を３つの行列の積へ分解する。
            X=USV^T
            U、Vは直行行列。直行行列は、転置行列と逆行列が等しくなる正方行列のこと。
            Sは対角行列で、対角成分には特異点が大きい順に並んでいる。大きいところだけを選ぶと、次元削減できる。対角成分以外はすべて０の行列。
        2.4.4PTBデータセット
            有名なコーパス。データセット。
3.word2vec
    3.1推論ベースの手法とニューラルネットワーク
        3.1.1カウントベースの手法の問題点
            語彙数100万とした場合、カウントベースでは100万×100万の巨大な行列を作ることになる。
        3.1.1推論ベースの手法の概要
            I ? a pen.　の?になにが入るかを学習する。Iとaを説明変数（コンテキスト）にして、?を目的変数（ターゲット）にする。
        3.1.3ニューラルネットワークにおける単語の処理方法
            one-hotベクトル
    3.2シンプルなword2vec
        3.2.1CBOWモデルの推論処理
            重みWinの各行には、それぞれの単語の分散表現が格納されている
        3.2.2CBOWモデルの学習
            MatMul→MatMul→Softmax→CrossEntropyError
        3.2.3word2vecの重みと分散表現
            入力側の重みだけ利用・・・これがポピュラー。
            出力側の重みだけ利用
            両方を利用
    3.5word2vecに関する補足
        3.5.2skip-gramモデル
            CBOWはIとaから?を予測。skip-gramはhaveからIとaを予測。
        3.5.3カウントベースvs推論ベース
            新しい語彙を足すとき、ミニバッチできる推論ベースが有利。
            word2vecはking-man+woman=queenが解ける。ただし、精度の優劣はつけられない。
            ２つの手法は関連がある・・・skip-gramとNegativeSamplingのモデルで、特殊な行列しているのと同じである。
            ２つの手法を融合・・・GloVe。コーパス全体の統計データの情報を損失関数に取り入れ、ミニバッチ学習する。
4.word2vecの高速化
    4.1word2vecの改良１
        語彙100万、中間層ニューロン数が100の場合。２箇所の計算がボトルネックになる。
            入力層のone-hot表現と重み行列Winの積による計算(4.1章で解決。Embeddingレイヤを導入して解決。)
            中間層の重み行列Woutの積およびSoftmaxレイヤの計算(4.2章で解決。Negative Samplingという新しい損失関数で解決。)
        4.1.1Embeddingレイヤ
            単語IDに該当する行を抜き出すレイヤを作る。そうすると、one-hot変換とMatMulレイヤでの行列の乗算は不要となる。
            MatMulレイヤをEmbeddingレイヤに切り替え、メモリ使用量を減らすことができる。
    4.2word2vecの改良２
        4.2.1中間層以降の計算の問題
            中間層のニューロンと重み行列Woutの積
            Softmaxレイヤの計算
        4.2.2多値分類から二値分類へ
            Woutには各単語IDの単語ベクトルが各列に格納されていて、haveという単語ベクトルを抽出し、そのベクトルと中間層のニューロンの内積を求めると、最終的なスコアを得られる。
        4.2.3シグモイド関数と交差エントロピー誤差
            二値分類をニューラルネットで解くには、スコアにシグモイド関数を適用し、確率を得る。
            損失関数は交差エントロピー誤差を使う。
            hとMatMul(Wout)とSoftmaxとクロスエントロピー誤差を、hとEmbedding(Oout)と内積とSoftmaxwithLoss、にする。hは中間層のニューロン。
        4.2.4多値分類から二値分類へ（実装編）
            MatMul→h→MatMul→Softmax→CrossEntropyError
                ↓
            Embed→h→EmbeddingDot→SigmoidwithLoss
        4.2.5NegativeSampling
            正しい答えhave以外の負例として、いくつかをピックアップする。（全部の語彙を負例とせず、節約）
            要するに、「すべて」の単語でなく「一部」の単語を対象とすることで、計算を効率化する。
            サンプリング方法は後述。
            Embed→h→EmbeddingDot→SigmoidwithLoss(正例と負例)
        4.2.6Negative Samplingのサンプリング手法
            ランダムにサンプリングするより、コーパスの統計データに基づいてサンプリングするほうがよい。
            具体的には、コーパスでよく使われている単語は抽出されやすく、そうでない単語は抽出されにくくする。
    4.3改良版word2vecの学習
    4.4word2vecに関する残りのテーマ
        単語の分散表現の利点は、単語を固定長のベクトルに変換できること。
        さらに、文章（単語の並び）についても、単語の分散表現を使って、固定長のベクトルに変換できる。
            例として、bag-of-wordsと呼ばれる単語の順序を考慮しないモデル。
        RNNを使えば、word2vecの単語の分散表現を利用しながら、文章を子手帳のベクトルに変換できる。
        固定長ベクトルに変換できることが重要であるのは、一般的な学習の手法（ニューラルネットやSVMなど）が適用できるから。
            自然言語→単語ベクトル化(word2vec)→一般的な学習の手法→答え
        4.4.2単語ベクトルの評価方法
            評価指標は、類似性や類推問題による評価。
            人が出したスコアとword2vecによるコサイン類似度のスコアを比較して、その相関を見る。
5.RNN
    5.1.確率と言語モデル
    5.2.RNNとは
        5.2.3.Backpropagation Through Time(BPTT)
        5.2.4.Truncated BPTT
            時間軸方向に長くなりすぎたネットワークを適当な場所で切り取り、小さなネットワークを複数作るというアイデア。
            逆伝播のつながりだけを切る。順伝播は切らない。
    5.3.RNNの実装
    5.4.時系列データを扱うレイヤの実装
        RNNLM(RNN Language Model)
        5.4.1.RNNLMの全体像
            Embedding→RNN→Affine→Softmax
        5.4.2.Timeレイヤの実装
            T個分の時系列データをまとめて処理するレイヤ
            TimeEmbedding→TimeRNN→TimeAffine,ts→TimeSoftmax
    5.5.RNNLMの学習と評価
        5.5.1.RNNLMの実装
        5.5.2.言語モデルの評価
            パープレキシティ
                確率の逆数。小さいほど良い。
6.ゲート付きRNN
    6.1.RNNの問題点
        6.1.2.勾配消失もしくは勾配爆発
        6.1.4.勾配爆発への対策
            勾配クリッピング
                閾値を使用
    6.2.勾配消失とLSTM
        勾配消失の対策としてLSTM,GRU
        LSTMのゲート
            input,forget,output
    6.3.LSTMの実装
        6.3.1.TimeLSTMの実装
            T個分の時系列データをまとめて処理するレイヤ
    6.4.LSTMを使った言語モデル
        TimeEmbedding→TimeLSTM→TimeAffine,ts→TimeSoftmaxwithLoss
    6.5.RNNLMのさらなる改善
        6.5.1.LSTMレイヤの多層化
            表現力を向上。
        6.5.2.Dropoutによる過学習の抑制
            正則化も有効
            時系列方向へのDropoutの挿入は、良くない。時間が進むにつれて情報が失われる。
            LSTM層とLSTM層の間にDropoutを挿入する。
            しかし最近「変分Dropout」で時間方向への適用も成功している。（同じ階層にあるDropoutでは共通のマスクを使う）
        6.5.3.重み共有
            Embedding→LSTM→LSTM→Affine→SoftmaxwithLoss (EmbeddingとAffineで重みを共有)
                学習パラメータを削減できる（学習が容易となる、過学習を抑制できる）。精度を向上できる。
        6.5.4.より良いRNNLMの実装
            ３つの改善を実装
7.RNNによる文章生成
    文章生成
    seq2seqの理解。（機械翻訳、自動要約、質疑応答、チャットボット、メールの返信、など）
    7.1.RNNによる文章生成
        7.1.1.RNNによる文章生成の手順
            これまでのモデルは、単語を与えたとき、次に出現する単語の確率分布を出力する。
            次の単語を新たに生成するにはどうするか？
                案１：最も確率の高い単語を選ぶ
                案２：確率的に選ぶ
                Iという単語を与え次の単語を生成、生成した単語を使って次の単語を生成、、、と行う。（7.2.1では、Iでなくeosを利用）
        7.1.3.さらに良い文章へ
            前章でパープレキシティを改良した。その実力を見る。
    7.2.seq2seq
        時系列データを別の時系列データに変換
        ２つのRNNを利用
        7.2.1.seq2seqの原理
            seq2seqはEncoder-Decoderモデルとも呼ばれる
            Encoder
                時系列データをhという隠れ状態ベクトルに変換
                LSTMレイヤの最後の隠れ状態
                重要な点は、hは固定長のベクトルである。
                任意の長さの文章を固定長のベクトルに変換することとなる。
                Embedding→LSTM→h
            Decoder
                Embedding,h→LSTM→Affine→Softmax
                hを受け取る以外は前節と同じだが、進化となる。
        7.2.2.時系列データ変換用のトイ・プロブレム
            足し算の例
        7.2.3.可変長の時系列データ
            パディング
    7.3.seq2seqの実装
    7.4.seq2seqの改良
        7.4.1.入力データの反転(Reverse)
        7.4.2.覗き見
            hをもっと活用
                DecoderのすべてのLSTMレイヤとAffineレイヤにhを与える
                Affineレイヤへの入力２本はconcatノードで結合して、Affineレイヤへ入力する
    7.5.seq2seqを用いたアプリケーション
        7.5.1.チャットボット
        7.5.2.アルゴリズムの学習
            ソースコード
        7.5.3.イメージキャプチョン
            画像を文章へ変換
            EncoderをLSTMからCNNに置き換えた
            CNNの出力の特徴マップは3次元なので、Affine変換で1次元に変換し、Decoderへ入力する。
8.Attention
    8.1.Attentionの仕組み
        必要な情報にだけ注意を向ける
        8.1.1.seq2seqの問題点
            固定長のベクトルが問題。
        8.1.2.Encoderの改良
            各時刻のLSTMレイヤの隠れ状態ベクトルをすべて利用する。入力文の長さに比例した情報をエンコードできるようになる。
            hsは単語の数だけベクトルがあり、それぞれのベクトルは各単語に対応した情報を多く含む（ここでは単方向LSTM）
        8.1.3.Decoderの改良１
            アライメント
                猫=catのような単語（またはフレーズ）の対応を表す情報
                Attentionはseq2seqに自動でアライメントのアイデアを取り込む
            Decoder
                Embedding,hs最終行→LSTM,hs→(何らかの計算)→Affine→Softmax
            (何らかの計算)で、吾輩に対応する単語はI、とhsから選び出す。
            選ぶという操作を、微分可能な演算に置き換える。
                すべてを選ぶ、ことで解決。各単語の重要度を表す重み(a)を別途計算する。(重みの求め方は8.1.4で後述)
                hsと重み(a)から、重み付き和を求める。その結果は「コンテキストベクトル(c)」。
                cには吾輩ベクトルを多く含む結果を得る。つまり、「吾輩」ベクトルを選ぶ操作を、(c)で代替していると言える。
        8.1.4.Decoderの改良２
            重みaを求める。
            Embedding,hs最終行→LSTM→h
            目指すのは、hがhsの各単語ベクトルと「どれだけ似ているか」を数値で表すこと。
            その数値化の方法が内積。
            内積の結果をsとする。（正規化前の値であり、スコアと呼ぶこともある）
            sを正規化するために、Softmax関数を適用する。s→a
            各単語の重みを求める計算グラフ
                h→hr,hs→t→s→a
        8.1.5.Decoderの改良３
            8.1.3ではWeightSum
            8.1.4ではAttentionWeight
            hs,h→AttentionWeight→a,hs→WeightSum→c
                Encoderが出力する各単語のベクトルhsに対してAttentionWeightレイヤが注意を払い、各単語の重みaを求める。
                それに続き、WeightSumレイヤがaとhsの重み付き和を求め、それをコンテキストベクトルcとして出力する。
                この一連の計算を行うレイヤをAttentionレイヤと呼ぶこととする。
            Embedding,hs最終行→LSTM,hs→Attention,LSTM→Affine→Softmax
                Attentionの情報（コンテキストベクトル）を追加することになる。
                元々あるLSTM（隠れ状態ベクトル）との連結したベクトルが、Affineに入力される。
    8.2.Attention付きseq2seqの実装
        AttentionEncoder,AttentionDecoder,AttentionSeq2seqクラスを実装
    8.3.Attentionの評価




