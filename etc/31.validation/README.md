# validation

違うモデルを比較する方法
    概要
        テストデータに対する性能で比較
            手法ごとに一つのモデル、つまり一つのハイパーパラメータの値、を事前に選んでおくのが一般的。
            ハイパーパラメータはクロスバリデーションで評価して決めるのが一般的。
        ダブルクロスバリデーションで比較（データが少ない場合）
    交差検証
        クロスバリデーション
            k-hold
            ハイパーパラメータの値ごとのモデルの推定性能を評価
        ダブルクロスバリデーション
            Leave-one-out
            複数の手法間でのモデルの推定性能を評価
        シャッフル分割交差検証
            ShuffleSplit
        グループ付き交差検証
            GroupKFold
    y-randomization
        ・モデル評価
            ・回帰、分類、どちらでも使用できる。
            ・目的変数 y の値をサンプル間でシャッフルして、回帰モデル・クラス分類モデルを構築する。
            これで、解析しているデータセットが、用いている回帰分析・クラス分類手法によって、どれくらい過学習 しやすいかを評価。
            ただ注意点として、ハイパーパラメータも、シャッフルした後の y のデータを使って、あらためて最適化。
            ・サンプル数が少ないときに問題となるのが、chance correlation (偶然の相関) 。
            y-randomization によって計算された r2, RMSE, MAE が、元の r2, RMSE, MAE と近いほど、
            元のデータセットでモデリングしたときに chance correlation が起きている可能性が高い。
            ・データ数が多い時、trainとtestで分けて評価。データが少ない時、y-randomizationを行い、
            r2 は 0 付近であり、RMSE, MAE が元のデータセットの結果より大きくなっていると、
            そのデータセットと あるアルゴリズム との組み合わせでは、chance correlation はほとんど起きていない。
        ・変数選択の評価にも利用できる。

回帰、分類
    回帰：上の実測値 vs. 推定値プロットの誤差が大きい原因
        比較指標
            R^2
            RMSE
            MAE
        モデルの適用範囲
            トレーニングデータに類似したサンプルがない。
            ADは回帰モデル・クラス分類モデルが本来の性能を発揮できるデータ領域のこと。
                凸包
                トレーニングデータの中心からの距離
                データ密度（SVM、K近傍）
                アンサンブル学習での分散がなければ適用
        回帰モデルが目的変数を説明できるほど学習できていない
            他の回帰分析手法も検討する必要がある。
        現状の記述子では目的変数を説明できない
            誤差の大きなサンプルと説明変数が類似したサンプルや、目的変数の値が近いサンプルを調べてみる
        目的変数の実測値が間違えている
            実測値データを確認する
    分類：
        比較指標
            正解率、精度、検出率だけでなく、混同行列 (confusion matrix)を確認
                False Negative (FN) と False Positive (FP) 
                FN を小さくしたいのか、FP を小さくしたいのか、どのくらいのバランスで両方を小さくしたいのかは、クラス分類モデルを用いる目的によって変わる
                混同行列
                    confusion_matrix
                    精度=TP+TN/TP+TN+FP+FN
                    適合率=TP/TP+FP
                        偽陰性の数を制限したい時に用いる。PPVとも呼ばれる。高価な臨床用の薬の節約。
                        陽性であると予測した内の何%が当たっていたかを示す。大きいほどよい。
                    再現率=TP/TP+FN 
                        実際に陽性のサンプルの内、陽性と予測されたものの割合。癌の人を全てみつけて、健康な人を癌と予測してもよい。
                        本当に陽性であるケースの内、何%を陽性と判定できたかを示す。大きいほどよい。
                    f=2*(適合率×再現率)/(適合率＋再現率) f-スコア、f-値。適合率と再現率をまとめる方法。
                    適合率、再現率、f-スコアの包括的なレポート
                        classification_report
        クラス分類におけるscoreingパラメータ値として重要な物
            accuracy
            ROC曲線
                X(偽陽性率)、Y(真陽性率)
                偽陽性
                roc_auc(ROCカーブの下領域(AUC))。
                    偏ったデータを評価する際にはAUCを用いることを勧める。
                    FPR=FP/FP+TN 偽陽性率。TPRは真陽性率(再現率の別名)。
                AUC(ROC曲線の下側面積)が大きいほどよい
            PR曲線
                X(再現率Recall)、Y(適合率Precision)
                average_precision(適合率-再現率カーブ下領域)
                AUPR(下面積)・・・適合率 + 再現率 の合計。
                歪みの大きい（陽性例より陰性例の方が圧倒的に多い）データを分析する際には、PR曲線を用いる。（PR曲線はROC曲線に比べ、よりランキング上位のサンプルの予測の正確さを重視するため）
                PR曲線のAUCの方が「ランキングが上位のサンプルの予測の正確さ」をより重視するのに対し，逆にROC曲線のAUCは「ランキング全体に渡る正確さ」を満遍なく評価している
            f1
            f1_macro
            f1_micro
            f1_weighted
        不確実性
            ほとんどのクラス分類器には予測の確実性を評価するためのdecision_functionメソッドもしくはpredict_probaメソッドが用意されている。
    説明変数xの重要度を確認できる手法
        決定木 (Decision Tree, DT)
        ランダムフォレスト (Random Forests, RF)
        勾配ブースティングGradient Boosting
            Gradient Boosting Decision Tree (GBDT)
            XGBoost
            LightGBM

サンプル数
    サンプル数が大きい時
        たとえば 10000 サンプルなど、サンプル数が大きくなると、カーネル関数を用いる手法が使えなくなるときがある。
        すべてのサンプル間においてカーネル関数で計算し、グラム行列を作成する必要があるためである。
        非常に大きな行列になり、メモリエラーになってしまう。
        たとえば Support Vector Regression (SVR) は難しい。
        非線形性を考慮したい場合は、決定木系や、(Deep) Neural Network 、を利用する。
    サンプル数が小さい時
        yランダマイゼーションでも結果がよくなってしまうか、チェックする。
        良くなってしまう時の対処法は、主観的に変数選択する。


# 参考
[データ解析に関するいろいろな手法・考え方・注意点のまとめ](https://datachemeng.com/summarydataanalysis/)  
[線形な手法とカーネル法（回帰分析）](https://qiita.com/wsuzume/items/09a59036c8944fd563ff)  
[【ROC曲線とAUC】機械学習の評価指標についての基礎講座](https://www.randpy.tokyo/entry/roc_auc)  
[AUCとAUPRを用いて予測性能を評価](https://mi-chan-nel.com/evaluation-of-prediction-performance/)  
[ROC曲線とPR曲線の違いについての考察](https://qiita.com/skyshk/items/016cd1820650ea78d101)  
