# 回帰

代表的なアルゴリズム
    KNeighborsRegressor
    線形モデル
        LinearRegression 通常最小二乗法。coef_係数、intercept_切片。
        Ridge
        Lasso
    DecisionTreeRegressor
    ランダムフォレスト
    勾配ブースティング
    ニューラルネットワーク（ディープラーニング）

非線形
    非線形の回帰分析手法
        手法
            Support Vector Regression (SVR)
            ランダムフォレスト (Random Forest, RF)
            ディープラーニングなど
        デメリット
            一般的に正しいかどうかはわからない。与えられたトレーニングデータ内では正しい、というだけ。
            実際の非線形性と回帰モデルの非線形性が異なるとき、モデルが外挿に対応できにくくなる。
            言い換えると、モデルの適用範囲・適用領域が狭くなる。
    非線形関係に対応するもう一つのアプローチ
        Xを非線形関数で変換して、それを新たなXとして用いる

モデルの過適合 (オーバーフィッティング) 問題を解決する手法
    主成分回帰 (Principal Component Regression, PCR)
        説明変数と目的変数との間で直接回帰分析をするのではなく、説明変数からPCAによって主成分を計算し、少数の主成分のみと目的変数との間で回帰分析を行う。このように主成分の数を制限することでモデルが複雑になることを防ぐ。さらに、主成分はデータセットのばらつきを表現できるように計算されるため、なるべく多くの情報を回帰分析に用いることができる。用いる主成分の数は解析者が決める必要がある。
    部分的最小二乗法 (Partial Least Squares, PLS)
        PLSでもPCRと同じように、説明変数から計算された主成分のうち、少数の主成分のみと目的変数との間で回帰分析を行う。PCRとの違いは、PCRでは主成分がデータセットの分散が大きくなるように計算されるのに対し、PLSでは主成分が目的変数との共分散が大きくなるように計算される。用いる主成分の数は解析者が決める必要がある。
    リッジ回帰 (Ridge Regression, RR)
        回帰係数の大きさに制限を加える。
        モデル構築用データセットの目的変数の誤差の二乗和のみを小さくするのではなく、回帰係数の二乗和も一緒に小さくしている。つまり、
        (誤差の二乗和)＋λ(回帰係数の二乗和)
        を最小化する。最小化するときの、誤差の二乗和の項に対する回帰係数の二乗和の項の比率であるλの値は解析者が決める必要がある。
    Least Absolute Shrinkage and Selection Operator (LASSO)
        略
    Elastic Net (EN)
        略
    サポートベクター回帰 (Support Vector Regression, SVR)
        回帰係数の大きさと、誤差の計算に使用するサンプルに制限を加えている。
        SVRはRRと似ている。誤差に関する項だけでなく、回帰係数の二乗和も一緒に小さくすることはRRと同じ。違いの一つは、SVRでは目的変数の誤差がある値 ε 以下のサンプルの誤差は0として扱う。これによって、すべてのサンプルの誤差を小さくするために回帰モデルが複雑になることを防ぐ。さらにSVRでは、誤差の二乗和ではなく誤差の絶対値の和を用いる。
        SVRでは、εとRRのλにあたるパラメータCの２つの値については解析者が設定する必要がある。

ガウス過程による回帰(Gaussian Process Regression, GPR)
    概要
        線形の回帰分析手法
        カーネルトリックにより非線形の回帰モデルに。
        リッジ回帰を拡張した非線形モデル。
        ガウス過程回帰は、リッジ回帰に「入力Xが似ていれば出力Yも似ている。」というガウス過程の特徴を制約条件として加えたもの。
        ガウス過程回帰が従うガウス分布は、リッジ回帰のような「等分散」を仮定しておらず、入力x, x'の距離(つまり似ているかどうか)に従って予測値y, y'の分散が変化する。
        確信度と判明している最大値のバランスをとって次の最適値を探索するベイズ最適化へ利用される。
        目的変数の推定値だけでなく、その分散も計算できる？
        クロスバリデーションがいらない（モデリングのときに最尤推定法でパラメータの値を決られる）？
    メリット
        非線形モデルが簡単に求められる。
            線形回帰やリッジ回帰では非線形のモデルを作成する場合、n次関数を複数用意して学習することで対応する。
            そのため、「何次の関数までを考慮すべきか」という点がネックとなり、モデル構築に労力を費やす。
            しかしガウス過程回帰では、「カーネルを用いる」ことでn次関数を用意せずに非線形モデルを求める。
            よって学習の際の計算プロセスやアルゴリズムは複雑化しますが、少ない労力で複雑なモデル構築ができる
        部分的な予測精度を確認できる
            ガウス過程の性質上、サンプル点の間隔が大きい箇所(横軸:8付近など)は予測がかなり外れる。
            しかし、そのような予測の外れている箇所は他の箇所と比較して「分散」が広く、ガウス過程自体が予測に自信がないと考えられる。
            つまりガウス過程は部分的な予測精度を確認できるモデルである。
    ガウス過程回帰で、カーネル関数をどうするか
        SVR では、カーネル関数におけるパラメータ (ガウシアンカーネルの γ) をクロスバリデーションで最適化する必要があるため、パラメータの数が多いことは、最適化に多くの時間がかかることであり望ましくない。GPR では、その必要はなく、モデリングのときに最尤推定法でパラメータの値を決めらレル。そのため、パラメータの数を多くしても計算時間的にそれほど問題ない。ただし、パラメータの数が多いとオーバーフィッティングの危険が増えることは注意。

混合ガウスモデルによる回帰（Gaussian Mixture Regression(GMR)）
    P（y｜x）を求めれば回帰分析
    P（x｜y）を求めれば逆解析


# 参考
[データ解析に関するいろいろな手法・考え方・注意点のまとめ](https://datachemeng.com/summarydataanalysis/)  
[線形の重回帰分析における予測性能を向上させる試み まとめ](http://univprof.com/archives/16-06-08-3768097.html)
[『ガウス過程と機械学習』Pythonのnumpyだけで実装するガウス過程回帰](https://qiita.com/ogi-iii/items/cf16e13ec09340016121)  
[一目で分かるRANSAC](https://qiita.com/kazetof/items/b3439d9258cc85ddf66b)
