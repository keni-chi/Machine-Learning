# NLP

## 概要

Transformer
    2017年の6月、 Attention Is All You Need という強いタイトルの論文が Google から発表
        (Attention は従来の RNN のモデル Seq2Seq などでも使われていました)
        Transformer は文章などのシーケンスから別の文章などのシーケンスを予測するモデルとして発表されました
    単語の位置をベクトルとして表現し入力単語に足すことで位置情報を付与(LSTMは直近だけ)
    シーケンス学習、とりわけNLPの分野では、最も多く選択されるニューラルネットワークアーキテクチャ、
    理由は、再帰型ニューラルネットワークアーキテクチャ(RNN)に対するアドバンテージの存在→セルフアテンション


Attention
    予測モデルに入力データのどの部分に注目するか知らせる機構のこと
    attention技術は機械翻訳への応用が特に有名
    単語同士の関係を行列で表す方法
    エンコーダーとデコーダーとして動作しています


セルフアテンション
    長所
        中でも、シーケンス内の以前の項目を"記憶"可能にするセルフアテンションメカニズムは、シーケンス全体の並列動作を可能にすることで、トレーニングや推論の高速化に寄与
        並列計算可能 (再帰型ニューラルネットワークと比べ)
        長文の為の深いモデル構築不要 (畳み込みニューラルネットワークと比べ)
    短所
        一方でセルフアテンションは、シーケンス内の各項目が他のすべての項目とリンク(あるいは"attend")する可能性があるため、その演算およびメモリの計算量は、処理される可能性のある最大シーケンス長をnとした場合、O(n^2)となる
        現在のハードウェアで処理可能なシーケンス長は、512項目程度が実用上の制限となっている。


BERT
    概要
        学習に残す単語は何がよいか、BERTは全体で予測用をランダム15%で検証。全体の行列計算。
        複数のタスクで人間を上回る性能を記録
            Transformerを複数結合させ、双方向で言語モデルを学習
        現在の自然言語処理では単語や文を意味を考慮した数値に変換させることが重要です。BERTはここで活躍。
        「Transformerによる双方向のエンコード表現」と訳され、2018年10月にGoogleのJacob Devlinらの論文で発表された自然言語処理モデル
    仕組み
        簡単な処理を追加
            ２つの文が連続するかどうか
                2つの文を結合させて入力し、出力される文のベクトルを使って連続かどうかのラベルを予測
            文の分類
                文を入力し、文ベクトルを使ってクラスラベルを予測
            質問応答
                質問文と知識結合させて入力し、知識のどこからどこまでが解答に当たるかを予測（解答の開始と終わりのクラスを予測）
            固有表現識別（人物、組織、場所など）
                各単語にその固有表現のクラスを予測する
    仕組み2
        BERTは入力されたシーケンスから別のシーケンスを予測
        BERTは事前学習モデルであり、入力されたラベルが付与されていない、つまり名前がついていない分散表現をTransformerが処理することによって学習
        2つの手法を同時進行で行うことで学習
            Masked Language Model
                双方向のTransformerによって学習するため、従来の手法に比べ精度が向上
                単語に関しての学習
            Next Sentence Prediction
                2つの入力文に対して「その2文が隣り合っているか」を当てるよう学習します。これにより、2つの文の関係性を学習
    特徴
        文脈理解
            「文脈を読むことが可能になった」
            Transformerというアーキテクチャ（構造）が組み込まれており、文章を双方向（文頭と文末）から学習することによって「文脈を読むこと」が実現
        汎用性
            従来のタスク処理モデルは特定のタスクにのみ対応しています。しかし、BERTはモデルの構造を修正せずとも、様々なタスクに応用
            既存のタスク処理モデルの前に接続（転移学習）するだけで、自然言語処理の精度を向上
        データ不足の克服
            従来のモデルとは違い、ラベルが付与されていないデータセットを処理できます
    学習
        マスク単語予測
            文中のランダムで選ばれた15％の単語をマスクして、それを予測します
            これにより単語の意味や単語同士の関係性を学習できると期待されます
        連続文予測
            2つの文が連続しているかどうかを予測
            これにより文の構造を学習できることが期待されます
    課題
        パラメーターが多く、層が厚い「巨大なモデル」
    以降
        そもそもマスクする単語はランダムでよいのか
        連続文の解釈は範囲が広すぎるため工夫できないか
        、など様々な工夫がされています


RoBERTA
    モデルアーキテクチャは BERT と全く同様の multi-head self-attention を採用
    もっとたくさん学習
    ・事前学習用データセットを大幅に増やした
    ・事前学習の回数を増やした
    もっとうまく学習
    ・バッチサイズを大きくした
    ・Next Sentence Prediction(NSP)を使用しない
    ・BERTは短い文章を多く投入していたが、RoBERTaでは長い文章を投入
    ・BERTは事前学習前に文章にマスクを行い、同じマスクされた文章を何度か繰り返していたが、RoBERTaでは、毎回ランダムにマスキングを行う


GPT-3
    OpenAIが開発しているGPT-3という言語モデルのAPIが一般に公開
    例文を読み込ませ文章生成
    他も生成
        「GPT-3」の凄さは「ニュースのような文章」以外も簡単に生成するところ。
        「ソースコードやデザイン」ですら「GPT-3」は理解し、生成。
        例
            Googleのトップページと同じデザインのソースコードがほしいとしましょう。
            簡単な命令を与えるだけで、「GPT-3」はGoogleとほぼ見た目が同じデザインとソースコードを再現しました。
        凄さ
            今までウェブのデザインや構築を行う際、「グーグルっぽいページ作って」といった命令文は「人間と人間だけの間で成り立つ会話」でした。
            かかった時間は数秒





# 参考
[作って理解する Transformer / Attention](https://qiita.com/halhorn/items/c91497522be27bde17ce)  
[今更ながらchainerでSeq2Seq（2）〜Attention Model編〜](https://qiita.com/kenchin110100/items/eb70d69d1d65fb451b67)  
[2019年はBERTとTransformerの年だった](https://ainow.ai/2020/02/25/183082/)  
[Longformer: The Long-Document Transformer](https://github.com/reo11/papers100knock/issues/35)  
[最近のDeep Learning (NLP) 界隈におけるAttention事情](https://www.slideshare.net/yutakikuchi927/deep-learning-nlp-attention)  
[自然言語処理でBERTまでの流れを簡単に紹介](https://nmoriyama.hatenablog.com/entry/2020/01/24/160351)  
[自然言語処理の王様「BERT」の論文を徹底解説](https://qiita.com/omiita/items/72998858efc19a368e50)
[「GPT-3」は思ってたより「やばい」ものだった](https://cubeglb.com/media/2020/07/22/gpt-3-gamechanger/)  


## 感情分析
[ML-Askでテキストの感情分析](https://qiita.com/yukinoi/items/ef6fb48b5e3694e9659c)  
[Pythonで日本語の感情分析(Google Colab)](https://qiita.com/konitech913/items/317b452cc6c63894fce3)  
