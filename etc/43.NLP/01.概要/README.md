# NLP

## 概要
覚書である。順次記載予定。  


2.自然言語と単語の分散処理
    2.2シソーラス
        類語辞書。car = auto, automobile....
        2.2.1WordNet
            1985年にスタートしたシソーラス。
        2.2.2シソーラスの問題点
            時代の変化に対応するのが困難
            人の作業コストが高い
            単語の細かなニュアンスを表現できない
    2.3カウントベースの手法
        コーパスの使用。大量のテキストデータのこと。
        2.3.2単語の分散表現
            単語の意味を的確にとらえたベクトル表現。RGBなど。
        2.3.3分布仮説
            単語の意味は周囲の単語によって形成される
            ウインドウサイズが１の場合、左右の１単語。
        2.3.4共起行列
            周囲の単語をカウント。カウントベースの手法、統計的手法、という。
        2.3.5ベクトル間の類似度
            コサイン類似度
    2.4カウントベースの手法の改善
        2.4.1相互情報量(PMI)
            theのように高頻度の単語は関連性が高くならないようにする。carとdriveの関連が強い結果になる。
        2.4.2次元削減
            特異点分解（SVD)。任意の行列を３つの行列の積へ分解する。
            X=USV^T
            U、Vは直行行列。直行行列は、転置行列と逆行列が等しくなる正方行列のこと。
            Sは対角行列で、対角成分には特異点が大きい順に並んでいる。大きいところだけを選ぶと、次元削減できる。対角成分以外はすべて０の行列。
        2.4.4PTBデータセット
            有名なコーパス。データセット。
3.word2vec
    3.1推論ベースの手法とニューラルネットワーク
        3.1.1カウントベースの手法の問題点
            語彙数100万とした場合、カウントベースでは100万×100万の巨大な行列を作ることになる。
        3.1.1推論ベースの手法の概要
            I ? a pen.　の?になにが入るかを学習する。Iとaを説明変数（コンテキスト）にして、?を目的変数（ターゲット）にする。
        3.1.3ニューラルネットワークにおける単語の処理方法
            one-hotベクトル
    3.2シンプルなword2vec
        3.2.1CBOWモデルの推論処理
            重みWinの各行には、それぞれの単語の分散表現が格納されている
        3.2.2CBOWモデルの学習
            MatMul→MatMul→Softmax→CrossEntropyError
        3.2.3word2vecの重みと分散表現
            入力側の重みだけ利用・・・これがポピュラー。
            出力側の重みだけ利用
            両方を利用
    3.5word2vecに関する補足
        3.5.2skip-gramモデル
            CBOWはIとaから?を予測。skip-gramはhaveからIとaを予測。
        3.5.3カウントベースvs推論ベース
            新しい語彙を足すとき、ミニバッチできる推論ベースが有利。
            word2vecはking-man+woman=queenが解ける。ただし、精度の優劣はつけられない。
            ２つの手法は関連がある・・・skip-gramとNegativeSamplingのモデルで、特殊な行列しているのと同じである。
            ２つの手法を融合・・・GloVe。コーパス全体の統計データの情報を損失関数に取り入れ、ミニバッチ学習する。
4.word2vecの高速化
    4.1word2vecの改良１
        語彙100万、中間層ニューロン数が100の場合。２箇所の計算がボトルネックになる。
            入力層のone-hot表現と重み行列Winの積による計算(4.1章で解決。Embeddingレイヤを導入して解決。)
            中間層の重み行列Woutの積およびSoftmaxレイヤの計算(4.2章で解決。Negative Samplingという新しい損失関数で解決。)
        4.1.1Embeddingレイヤ
            単語IDに該当する行を抜き出すレイヤを作る。そうすると、one-hot変換とMatMulレイヤでの行列の乗算は不要となる。
            MatMulレイヤをEmbeddingレイヤに切り替え、メモリ使用量を減らすことができる。
    4.2word2vecの改良２
        4.2.1中間層以降の計算の問題
            中間層のニューロンと重み行列Woutの積
            Softmaxレイヤの計算
        4.2.2多値分類から二値分類へ
            Woutには各単語IDの単語ベクトルが各列に格納されていて、haveという単語ベクトルを抽出し、そのベクトルと中間層のニューロンの内積を求めると、最終的なスコアを得られる。
        4.2.3シグモイド関数と交差エントロピー誤差
            二値分類をニューラルネットで解くには、スコアにシグモイド関数を適用し、確率を得る。
            損失関数は交差エントロピー誤差を使う。
            hとMatMul(Wout)とSoftmaxとクロスエントロピー誤差を、hとEmbedding(Oout)と内積とSoftmaxwithLoss、にする。hは中間層のニューロン。
        4.2.4多値分類から二値分類へ（実装編）
            MatMul→h→MatMul→Softmax→CrossEntropyError
                ↓
            Embed→h→EmbeddingDot→SigmoidwithLoss
        4.2.5NegativeSampling
            正しい答えhave以外の負例として、いくつかをピックアップする。（全部の語彙を負例とせず、節約）
            要するに、「すべて」の単語でなく「一部」の単語を対象とすることで、計算を効率化する。
            サンプリング方法は後述。
            Embed→h→EmbeddingDot→SigmoidwithLoss(正例と負例)
        4.2.6Negative Samplingのサンプリング手法
            ランダムにサンプリングするより、コーパスの統計データに基づいてサンプリングするほうがよい。
            具体的には、コーパスでよく使われている単語は抽出されやすく、そうでない単語は抽出されにくくする。
    4.3改良版word2vecの学習
    4.4word2vecに関する残りのテーマ
        単語の分散表現の利点は、単語を固定長のベクトルに変換できること。
        さらに、文章（単語の並び）についても、単語の分散表現を使って、固定長のベクトルに変換できる。
            例として、bag-of-wordsと呼ばれる単語の順序を考慮しないモデル。
        RNNを使えば、word2vecの単語の分散表現を利用しながら、文章を子手帳のベクトルに変換できる。
        固定長ベクトルに変換できることが重要であるのは、一般的な学習の手法（ニューラルネットやSVMなど）が適用できるから。
            自然言語→単語ベクトル化(word2vec)→一般的な学習の手法→答え
        4.4.2単語ベクトルの評価方法
            評価指標は、類似性や類推問題による評価。
            人が出したスコアとword2vecによるコサイン類似度のスコアを比較して、その相関を見る。
5.RNN
    5.1.確率と言語モデル
    5.2.RNNとは
        5.2.3.Backpropagation Through Time(BPTT)
        5.2.4.Truncated BPTT
            時間軸方向に長くなりすぎたネットワークを適当な場所で切り取り、小さなネットワークを複数作るというアイデア。
            逆伝播のつながりだけを切る。順伝播は切らない。
    5.3.RNNの実装
    5.4.時系列データを扱うレイヤの実装
        RNNLM(RNN Language Model)
        5.4.1.RNNLMの全体像
            Embedding→RNN→Affine→Softmax





## 参考

