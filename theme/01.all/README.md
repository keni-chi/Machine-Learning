# 全般

最終的なモデル
    最終的に用いるモデルは、トレーニグデータもテストデータも含むすべてのデータを用いて構築されたモデルである。
    トレーニグデータでは良好なモデルが構築できたことが前提で、最終的なモデルを構築するときも、y-randomization やモデルの適用範囲を検討する必要はある。

過学習を防ぐ方法：大きく分けて３つ
    説明変数 (入力変数・記述子・特徴量) の数を減らしたり、データ量を圧縮したりする。
    過学習するときには、係数の値が大きくなりがちなので、Ridgeなどの手法を利用する。
    学習を途中でストップする（テスト用のデータセットの誤差が十分に小さくなるか、クロスバリデーション、などでチェック）

アンサンブル学習
    概要
        複数の異なるモデルを構築して、推定するときはそれらのモデルの推定結果を統合するのがアンサンブル学習。
    異なるモデルの構築方法
        サンプルや説明変数 (記述子・特徴量・入力変数) を変えてモデルを作る。
        サンプルからではなく、説明変数から選ぶときは、同じ変数があっても無意味なので、ジャックナイフ法を使う必要がある。
            重複を許してサンプルを選ぶ方法：ブートストラップ法 (bootstrap resampling or bootstrapping)。bagging (バギング) 。
                ランダムフォレスト
                    例として、決定木 (Decision Tree, DT) をアンサンブル学習すると、ランダムフォレスト (Random Forests, RF) になる。
                    ランダムフォレストでは、サンプルをブートストラップ法で選び、同時に説明変数をジャックナイフ法で選ぶことで、サブデータセットを作成し、サブモデルとしての決定木をつくっている。
            重複を許さずサンプルを選ぶ方法：ジャックナイフ法 (Jackknife resampling or jackknifing)
    メリット
        外れ値やノイズに対してロバストな推定ができる
            １つのモデルの場合、外れ値やノイズの影響を受けたモデルとなり、新しいサンプルの推定のとき、推定を失敗することもある。アンサンブル学習により、リサンプリングしてたくさんモデルを作ることで、外れ値の影響を受けたサブモデルだけでなく、(あまり)受けていないサブモデルもできる。最後に多数決や平均値・中央値を求めることで、外れ値の影響を減らせる。ノイズについても、推定値が平均化されることでばらつきが軽減できる。
        推定値のバイアスが減る
            Adaptive Boosting。通常のバギングではなく、これまでのサブモデルで推定に失敗したサンプルほど高確率で選ばれるようにする。
        推定値の不確かさ (モデルの適用範囲・適用領域) を考慮できる。
            分類
                多数決だけでなく、その内訳まで確認し、不確かさを考慮。
            回帰
                推定値の標準偏差を指標にする。推定値の標準偏差、つまり推定値のばらつきが小さいときは、平均値・中央値は推定値として確からしいだろう、逆に大きいときはその分平均値や中央値から実測値がズレる可能性もあるだろう、と考える。
    デメリット
        計算時間がかかる。サブモデルをたくさん構築し、各サブモデルでハイパーパラメータを最適化しなければならない。

ブースティング
    概要
        データの一部を抽出してそれで弱学習機を作り、最後に合わせるのはバギングと同様。違いは前回の結果を利用するのがブースティング。なので並列処理はできない。
    手法
        アダブースト
            前回の結果で誤分類された値の重みを大きくするように更新する。ブースティングと単に言えばこれを指すことが多い。
        勾配ブースティング
            概要
                アンサンブル学習の１つ。
                ブースティングの一つ。
                回帰、分類、どちらも対応。
                誤って分類されたサンプル、誤差の大きかったサンプルを改善するように（損失関数の値が小さくなるように）新たなモデルが構築される。
                勾配降下法を使ったブースティングのこと。アダブーストとの違いがよくわからない。
            GBDT(Gradient Boosting Decision Tree)
                弱学習機に決定木を使った勾配ブースティングのこと。
            XGBoost
            LightGBM

カーネル
    カーネル法
        生データに非線形写像を施して、それを新しいデータとみなして線形な手法を適用し、分析する。
        カーネル法を使うと、線形な手法で培ってきた知識をほとんどそのまま生かして非線形なデータを扱うことができる。
        また、カーネル法はブラックボックスとはならない。

    カーネル関数
        概要
            サンプル間の類似度を評価するもの。（サンプル間の内積以外も）
        例
            線形カーネル
            ガウシアンカーネル
            多項式カーネル
        アルゴリズム
            サポートベクターマシン (Support Vector Machine, SVM)  
            サポートベクター回帰 (Support Vector Regression, SVR) 
            ガウス過程回帰 (Gaussian Process Regression, GPR) など

    カーネル回帰
        概要
            一定の条件さえ満たせば自由に設計することができる。
            カーネル法で回帰を行うためにはまずグラム行列を計算する
            グラム行列は愚直に各成分ごとに計算するしかなく、O(N^2)の計算量を要するので、大規模なデータ分析にカーネル法を用いる場合はネックとなる
            よく使われるのはガウスカーネル。
                k(x,x′)=exp(−β∥x−x′∥^2) βはβ>0を満たす実数のハイパーパラメータ.
                重みαについて、制約が無ければ過学習となる。制約として、二乗誤差に正則化項と呼ばれる項を付け加えて最適化すると、Ridge回帰となる（λはハイパーパラメータ）。

距離
    ユークリッド距離 (Euclidian distance)
        人が定規で測るような距離
    マンハッタン距離 (Manhattan distance or City block distance)
        碁盤の目のように区画された道しか通れない状況で測るような距離
    チェビシェフ距離 (Chebyshev distance)
        各変数の値の差について、絶対値の最大値を距離とする。
        重要でないノイズのような変数も含まれており、ノイズの影響を抑えたいときには、チェビシェフ距離を用いる。
    マハラノビス距離 (Mahalanobis distance)
        変数間の相関関係が (正でも負でも) 強いときに有効な距離
        変数間の相関関係を考慮しながら距離が計算される
        変数間に相関のあるようなデータセットの場合は、マハラノビス距離を用いるとよい

最適化
    概要
        数理最適化 - 最適化問題のうち解が連続的なもの。
        組合せ最適化 - 最適化問題のうち、解が離散的なもの。
            ・遺伝的アルゴリズム
            ・タブーサーチ
            ・シミュレーテッドアニーリング
    応用
        時間や距離を最小化することで最適化
            ・シフト・工程最適化
            ・ルート・経路最適化

遺伝的アルゴリズム
    概要
        シャッフルを行なって、次世代の遺伝子を生成する。
        そして、再び採点をして、上位20％を残して、さらに子孫を残す。
        これを繰り返していくと、何世代後かには得点が延びなくなっていきます。これが収束で、解が求められるようになる。
    流れ
        1.あらかじめN個の個体が入る集合を二つ用意する。以下、この二つの集合を「現世代」、「次世代」と呼ぶことにする。
        2. 現世代にN個の個体をランダムに生成する。
        3. 評価関数により、現世代の各個体の適応度をそれぞれ計算する。
        4. ある確率で次の3つの動作のどれかを行い、その結果を次世代に保存する。
            個体を二つ選択（選択方法は後述）して交叉（後述）を行う。
                クロスオーバー。交叉（組み換え）。
                    ２つの遺伝子が父親と母親になって子孫を作る。
                    それぞれの遺伝子は、○と×が並んだ構成になっている。あるポイントで、そっくり遺伝子を入れ替える。
            個体を一つ選択して突然変異（後述）を行う。
                突然変異
                    遺伝子がランダムに変わってしまう現象
            個体を一つ選択してそのままコピーする。
        5. 次世代の個体数がN個になるまで上記の動作を繰り返す。
        6. 次世代の個体数がN個になったら次世代の内容を全て現世代に移す。
        7. 3.以降の動作を最大世代数G回まで繰り返し、最終的に「現世代」の中で最も適応度の高い個体を「解」として出力する。
    問題点
        過剰適応
            初期収束とは、最初の方の世代で「偶然」他の個体より適応度が圧倒的に高い個体が生まれたとき、その個体の遺伝子が集団中に爆発的に増えて探索がかなり早い段階で収束してしまう現象である。ルーレット選択の設定が甘い場合や、突然変異の効果が上手く表れないときに起こりやすい。
        ヒッチハイク問題
            最適解と一致するビットの近くにいて最適解の生成を妨げる現象
            ３つの遺伝子が端のふたつは正解なのに、真ん中が不正解という場合、この不正解の遺伝子は、周りの正解遺伝子に守られて、なかなか淘汰されない。
    ライブラリ
        VCOPT



# 参考
[データ解析に関するいろいろな手法・考え方・注意点のまとめ](https://datachemeng.com/summarydataanalysis/)  
[アンサンブル学習のブースティングとバギングの違いについて](https://analytics-and-intelligence.net/archives/678)  
[線形な手法とカーネル法（回帰分析）](https://qiita.com/wsuzume/items/09a59036c8944fd563ff)  
[遺伝的アルゴリズム](https://qiita.com/YuichiroMinato/items/f8d941675946de7db588)  
