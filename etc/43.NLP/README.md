# NLP

## 概要
覚書である。順次記載予定。  

# 参考
[作って理解する Transformer / Attention](https://qiita.com/halhorn/items/c91497522be27bde17ce)  
[今更ながらchainerでSeq2Seq（2）〜Attention Model編〜](https://qiita.com/kenchin110100/items/eb70d69d1d65fb451b67)  
[2019年はBERTとTransformerの年だった](https://ainow.ai/2020/02/25/183082/)  
[Longformer: The Long-Document Transformer](https://github.com/reo11/papers100knock/issues/35)
[最近のDeep Learning (NLP) 界隈におけるAttention事情](https://www.slideshare.net/yutakikuchi927/deep-learning-nlp-attention)  


