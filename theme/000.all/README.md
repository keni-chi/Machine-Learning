# 全般

アンサンブル学習
    概要
        複数の異なるモデルを構築して、推定するときはそれらのモデルの推定結果を統合するのがアンサンブル学習。
    異なるモデルの構築方法
        サンプルや説明変数 (記述子・特徴量・入力変数) を変えてモデルを作る。
        サンプルからではなく、説明変数から選ぶときは、同じ変数があっても無意味なので、ジャックナイフ法を使う必要がある。
            重複を許してサンプルを選ぶ方法：ブートストラップ法 (bootstrap resampling or bootstrapping)。bagging (バギング) 。
                ランダムフォレスト
                    例として、決定木 (Decision Tree, DT) をアンサンブル学習すると、ランダムフォレスト (Random Forests, RF) になる。
                    ランダムフォレストでは、サンプルをブートストラップ法で選び、同時に説明変数をジャックナイフ法で選ぶことで、サブデータセットを作成し、サブモデルとしての決定木をつくっている。
            重複を許さずサンプルを選ぶ方法：ジャックナイフ法 (Jackknife resampling or jackknifing)
    メリット
        外れ値やノイズに対してロバストな推定ができる
            １つのモデルの場合、外れ値やノイズの影響を受けたモデルとなり、新しいサンプルの推定のとき、推定を失敗することもある。アンサンブル学習により、リサンプリングしてたくさんモデルを作ることで、外れ値の影響を受けたサブモデルだけでなく、(あまり)受けていないサブモデルもできる。最後に多数決や平均値・中央値を求めることで、外れ値の影響を減らせる。ノイズについても、推定値が平均化されることでばらつきが軽減できる。
        推定値のバイアスが減る
            Adaptive Boosting。通常のバギングではなく、これまでのサブモデルで推定に失敗したサンプルほど高確率で選ばれるようにする。
        推定値の不確かさ (モデルの適用範囲・適用領域) を考慮できる。
            分類
                多数決だけでなく、その内訳まで確認し、不確かさを考慮。
            回帰
                推定値の標準偏差を指標にする。推定値の標準偏差、つまり推定値のばらつきが小さいときは、平均値・中央値は推定値として確からしいだろう、逆に大きいときはその分平均値や中央値から実測値がズレる可能性もあるだろう、と考える。
    デメリット
        計算時間がかかる。サブモデルをたくさん構築し、各サブモデルでハイパーパラメータを最適化しなければならない。

ブースティング
    概要
        データの一部を抽出してそれで弱学習機を作り、最後に合わせるのはバギングと同様。違いは前回の結果を利用するのがブースティング。なので並列処理はできない。
    手法
        アダブースト
            前回の結果で誤分類された値の重みを大きくするように更新する。ブースティングと単に言えばこれを指すことが多い。
        勾配ブースティング
            概要
                アンサンブル学習の１つ。
                ブースティングの一つ。
                回帰、分類、どちらも対応。
                誤って分類されたサンプル、誤差の大きかったサンプルを改善するように（損失関数の値が小さくなるように）新たなモデルが構築される。
                勾配降下法を使ったブースティングのこと。アダブーストとの違いがよくわからない。
            GBDT(Gradient Boosting Decision Tree)
                弱学習機に決定木を使った勾配ブースティングのこと。
            XGBoost
            LightGBM

カーネル法
    生データに非線形写像を施して、それを新しいデータとみなして線形な手法を適用し、分析する。
    カーネル法を使うと、線形な手法で培ってきた知識をほとんどそのまま生かして非線形なデータを扱うことができる。
    また、カーネル法はブラックボックスとはならない。

カーネル関数
    概要
        サンプル間の類似度を評価するもの。（サンプル間の内積以外も）
    例
        線形カーネル
        ガウシアンカーネル
        多項式カーネル
    アルゴリズム
        サポートベクターマシン (Support Vector Machine, SVM)  
        サポートベクター回帰 (Support Vector Regression, SVR) 
        ガウス過程回帰 (Gaussian Process Regression, GPR) など

カーネル回帰
    概要
        一定の条件さえ満たせば自由に設計することができる。
        カーネル法で回帰を行うためにはまずグラム行列を計算する
        グラム行列は愚直に各成分ごとに計算するしかなく、O(N^2)の計算量を要するので、大規模なデータ分析にカーネル法を用いる場合はネックとなる
        よく使われるのはガウスカーネル。
            k(x,x′)=exp(−β∥x−x′∥^2) βはβ>0を満たす実数のハイパーパラメータ.
            重みαについて、制約が無ければ過学習となる。制約として、二乗誤差に正則化項と呼ばれる項を付け加えて最適化すると、Ridge回帰となる（λはハイパーパラメータ）。




# 参考
[データ解析に関するいろいろな手法・考え方・注意点のまとめ](https://datachemeng.com/summarydataanalysis/)  
[アンサンブル学習のブースティングとバギングの違いについて](https://analytics-and-intelligence.net/archives/678)  
[線形な手法とカーネル法（回帰分析）](https://qiita.com/wsuzume/items/09a59036c8944fd563ff)  
