# 変数選択

全般
    標準偏差(分散)が０の説明変数を削除して良いか。
        回帰モデルやクラス分類モデルを構築するときには、変数を削除してしまって問題なし。
        しかし、モデルの適用範囲 (適用領域) Applicability Domain (AD) を考えるときは異なる。
        理由は、標準偏差が 0 になったのはトレーニングデータの中だけであって、テストデータまで含めると、標準偏差が 0 になるとは限らないから。
    ビニング(離散化)
        機械学習を行う上で、連続データの特徴量が不足している場合、線形モデルでは上手く学習することができないケースがある。特徴量の範囲を一定の数に分割することで、それぞれのセクションごとに学習をさせる。

手法
    Stepwise法
        説明変数 (入力変数・記述子・特徴量) を選択する手法
        １つずつ説明変数を追加したり、削除したりしながら、最適な説明変数の組合せを探す
        回帰モデルの構築を繰り返す数が多くなると時間がかかる
        どんな回帰分析手法とも組み合わせることができる
        Scikit-learnにstepwise法は無い。
        計算量が多い
        選んだ特徴量が過学習する
    Boruta
        ランダムフォレスト (Random Forest, RF) の変数重要度に基づく変数選択手法
        ニセの特徴量のshadowを作って重要度を比較する
    Genetic Algorithm-based Partial Least Squares (GAPLS)
        遺伝的アルゴリズム (GA) と PLS とを組み合わせた変数選択手法
        PLS でクロスバリデーションを行ったときの r2 が大きくなるように変数が選択される
    Genetic Algorithm-based Support Vector Regression (GASVR)
        GA と SVR とを組み合わせた変数選択手法
        SVR でクロスバリデーションを行ったときの r2 が大きくなるように変数が選択される
        染色体にSVRのハイパーパラメータも入れることで、高速化している
    lasso
        選んだ特徴量が過学習する
        ロジスティクス回帰やSVMで用いる？
    単変量統計
        個々の特徴量とターゲットとの間に統計的に顕著な関係があるかどうかを計算する。複数特徴量の組み合わせは逃してしまう。
            SelectKBest
            SelectPecentile
    モデルベース選択
        教師あり学習モデルを用いて個々の特徴量の重要性を判断し、重要な物だけを残す手法。
        SelectFromModel
    反復選択
        再帰的特徴量削減RFE 全ての特徴量から開始してモデルを作り、そのモデルで最も重要度が低い特徴量を削除する。これを事前に定めた数の特徴量になるまで繰り返す。
    その他
        相関係数で変数選択
        相関係数で変数のクラスタリング

評価
    y-randomizationで評価



# 参考
[データ解析に関するいろいろな手法・考え方・注意点のまとめ](https://datachemeng.com/summarydataanalysis/)  

